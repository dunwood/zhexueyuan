import requests
from bs4 import BeautifulSoup
import os
import re
import sys
import random
from datetime import datetime

# --- åŸºç¡€é…ç½® ---
BASE_DIR = "articles" 
IMAGE_SUBDIR = "images" 
VIDEO_SUBDIR = "videos" 
INDEX_FILE = "index.html" 

def download_and_sync():
    print("=== å“²å­¦å›­å…¨è‡ªåŠ¨æ›´æ–°ï¼šç»ˆæå…¼å®¹ç‰ˆ ===")

    # 1. è·å–å‚æ•°
    if len(sys.argv) > 1 and sys.argv[1].strip():
        url = sys.argv[1].strip()
        category_id = sys.argv[2].strip() if len(sys.argv) > 2 else "laochan-column"
        print(f"ğŸ”— å¤„ç†é“¾æ¥: {url}")
        print(f"ğŸ“‚ ç›®æ ‡åˆ†ç±»: {category_id}")
    else:
        print("âŒ é”™è¯¯ï¼šæœªæ¥æ”¶åˆ°æœ‰æ•ˆçš„æ–‡ç« é“¾æ¥")
        return

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
    }

    try:
        res = requests.get(url, headers=headers)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # --- 2. æŠ“å–æ ‡é¢˜ ---
        title_tag = soup.find('h1', class_='rich_media_title') or soup.find('h1')
        title = title_tag.get_text().strip() if title_tag else "æœªå‘½åæ–‡ç« "
        # ç§»é™¤éæ³•æ–‡ä»¶åå­—ç¬¦
        safe_title = re.sub(r'[\\/:*?\"<>|]', '', title).strip()
        # --- 2. è§£æå†…å®¹ ---
        title_tag = soup.find('h1', class_='rich_media_title')
        if not title_tag:
            print("âŒ æ— æ³•è§£ææ–‡ç« æ ‡é¢˜ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ˜¯å¦ä¸ºæ ‡å‡†çš„å¾®ä¿¡æ–‡ç« ")
            return
        title = title_tag.get_text(strip=True)
        date_str = datetime.now().strftime('%Y-%m-%d')

        # --- 3. å¼ºåŒ–ç‰ˆæ­£æ–‡å®šä½ (è§£å†³å¼€å¤´æ®µè½ä¸¢å¤±é—®é¢˜) ---
        main_area = soup.find('div', id='js_content') or \
                    soup.find('div', class_='rich_media_content')
        
        if not main_area:
            print("âŒ æŠ“å–å¤±è´¥ï¼šæ‰¾ä¸åˆ°æ–‡ç« æ­£æ–‡åŒºåŸŸã€‚")
        content_area = soup.find('div', class_='rich_media_content')
        if not content_area:
            print("âŒ æ— æ³•è·å–æ–‡ç« æ­£æ–‡å†…å®¹")
            return

        # åˆ›å»ºèµ„æºç›®å½•
        os.makedirs(os.path.join(BASE_DIR, IMAGE_SUBDIR, safe_title), exist_ok=True)
        os.makedirs(os.path.join(BASE_DIR, VIDEO_SUBDIR, safe_title), exist_ok=True)
        # æ¸…ç†æ— ç”¨æ ·å¼
        for tag in content_area.find_all(True):
            if tag.name not in ['img', 'video', 'iframe']:
                tag.attrs = {}

        md_content = f"# {title}\n\n---\n\n"
        img_count = 0
        # --- 3. å‡†å¤‡æ–‡ä»¶å¤¹ (æ”¯æŒå¤šçº§ç›®å½•) ---
        # æ ¸å¿ƒä¿®æ”¹ï¼šå°† ID ä¸­çš„ / è½¬æ¢ä¸ºç³»ç»Ÿè·¯å¾„æ–œæ ï¼Œå®ç°å¤šçº§ç›®å½•è‡ªåŠ¨åˆ›å»º
        category_path = category_id.replace('/', os.sep)
        md_file_dir = os.path.join(BASE_DIR, category_path)
        img_dir = os.path.join(md_file_dir, IMAGE_SUBDIR)

        # éå†æ‰€æœ‰ç›´æ¥å­å…ƒç´ ï¼Œç¡®ä¿ä¸æ¼æ‰ä»»ä½• section æˆ– div åŒ…è£¹çš„å¼€å¤´
        for element in main_area.find_all(True, recursive=False):
            # å¤„ç†å›¾ç‰‡
            all_imgs = element.find_all('img')
            for img in all_imgs:
                src = img.get('data-src') or img.get('src')
                if src and src.startswith('http'):
                    try:
                        img_res = requests.get(src, timeout=15)
                        if len(img_res.content) > 5000:
                            img_count += 1
                            img_name = f"{img_count}.jpg"
                            img_path = os.path.join(BASE_DIR, IMAGE_SUBDIR, safe_title, img_name)
                            with open(img_path, 'wb') as f:
                                f.write(img_res.content)
                            md_content += f"![å›¾ç‰‡](articles/images/{safe_title}/{img_name})\n\n"
                    except: pass
        if not os.path.exists(img_dir):
            os.makedirs(img_dir, exist_ok=True)
            print(f"ğŸ“‚ å·²åˆ›å»ºç›®å½•: {img_dir}")

            # å¤„ç†æ–‡å­—ï¼šä½¿ç”¨ separator ç¡®ä¿åµŒå¥—æ–‡å­—ä¸ç²˜è¿
            text = element.get_text(separator="\n", strip=True)
            if text:
                if "æ‰«æäºŒç»´ç " not in text and "é˜…è¯»åŸæ–‡" not in text:
                    # åªæœ‰å½“å—å†…æ–‡å­—ä¸æ˜¯çº¯å›¾ç‰‡å ä½ç¬¦æ—¶æ‰æ·»åŠ 
                    if len(text) > 1 or not all_imgs:
                        md_content += f"{text}\n\n"
        # --- 4. å¤„ç†å›¾ç‰‡å¹¶ç”Ÿæˆ Markdown ---
        img_count = 0
        for img in content_area.find_all('img'):
            src = img.get('data-src') or img.get('src')
            if src:
                img_count += 1
                img_name = f"img_{img_count}.jpg"
                img_path = os.path.join(img_dir, img_name)
                try:
                    img_res = requests.get(src, headers=headers)
                    with open(img_path, 'wb') as f:
                        f.write(img_res.content)
                    # ç½‘é¡µæ˜¾ç¤ºçš„ç›¸å¯¹è·¯å¾„ï¼šä½¿ç”¨æ­£æ–œæ  /
                    web_img_path = f"{IMAGE_SUBDIR}/{img_name}"
                    img.replace_with(f"\n\n![å›¾ç‰‡]({web_img_path})\n\n")
                except:
                    print(f"âš ï¸ å›¾ç‰‡ {src} ä¸‹è½½å¤±è´¥")

            # å¤„ç†è§†é¢‘å ä½
            html_str = str(element)
            if 'finder_video_card' in html_str or 'video' in html_str or element.find('iframe'):
                video_rel_path = f"articles/videos/{safe_title}/video.mp4"
                if video_rel_path not in md_content:
                    md_content += f'\n<div style="text-align:center;"><video src="{video_rel_path}" controls style="max-width:100%"></video></div>\n\n'
        md_content = f"# {title}\n\n"
        md_content += f"> å‘å¸ƒæ—¥æœŸ: {date_str}\n\n"
        md_content += content_area.get_text(separator="\n\n")

        # --- 4. ä¿å­˜ Markdown æ–‡ä»¶ ---
        md_file_dir = os.path.join(BASE_DIR, category_id)
        os.makedirs(md_file_dir, exist_ok=True)
        safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
        md_file_path = os.path.join(md_file_dir, f"{safe_title}.md")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"ğŸ“ Markdown å·²ç”Ÿæˆ: {md_file_path}")

       # --- 5. åŒæ­¥æ›´æ–° index.html (ç²¾å‡†æ’å…¥é€»è¾‘) ---
        # --- 5. åŒæ­¥æ›´æ–° index.html (ç²¾å‡†æ’å…¥é€»è¾‘) ---
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            index_content = f.read()

        # æ£€æŸ¥æ˜¯å¦é‡å¤
        if f"'{title}'" in index_content or f'"{title}"' in index_content:
        if f"'{title}'" in index_content or f'\"{title}\"' in index_content:
            print(f"âš ï¸ é¦–é¡µåˆ—è¡¨ä¸­å·²å­˜åœ¨ã€Š{title}ã€‹ï¼Œè·³è¿‡æ’å…¥ã€‚")
        else:
            # ç½‘é¡µè®¿é—®è·¯å¾„ç»Ÿä¸€ç”¨æ­£æ–œæ 
            md_path_web = f"articles/{category_id}/{safe_title}.md"
            # ç”Ÿæˆå”¯ä¸€ ID
            article_id = f"art_{datetime.now().strftime('%H%M%S')}{random.randint(100, 999)}"
            new_entry = f"{{ id: '{article_id}', title: '{title}', filePath: '{md_path_web}', date: '{date_str}' }},"

            # æ ¸å¿ƒæ­£åˆ™è¡¨è¾¾å¼ï¼šåªæ‰¾ [ ç¬¦å·
            pattern = rf"(['\"]?{category_id}['\"]?\s*:\s*\[)"
            # æ ¸å¿ƒä¿®æ”¹ï¼šä½¿ç”¨ re.escape å¤„ç† category_idï¼Œä½¿å…¶æ”¯æŒ translated-work/reasoning ä¸­çš„æ–œæ 
            pattern = rf"(['\"]{re.escape(category_id)}['\"]\s*:\s*\[)"

            if re.search(pattern, index_content):
                # å°±åœ¨ [ åé¢æ¢è¡Œæ’å…¥æ–°å†…å®¹ï¼Œè¿™æ ·æ—¢ä¸å½±å“è€æ–‡ç« ï¼Œä¹Ÿèƒ½å¡«æ»¡ç©ºæ‹¬å·
                index_content = re.sub(pattern, f"\\1\n                {new_entry}", index_content)
                
                # --- 5. åŒæ­¥æ›´æ–° index.html (æœ€ç¨³ä¸‡èƒ½åŒ¹é…é€»è¾‘) ---
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            index_content = f.read()

        # æ ¸å¿ƒæ­£åˆ™ï¼šåŒ¹é… 'åˆ†ç±»å': [
        # å¢åŠ äº† \s* å®¹é”™å¤„ç†ï¼Œå¹¶ä½¿ç”¨ re.IGNORECASE å¿½ç•¥å¤§å°å†™
        pattern = rf"(['\"]?\s*{category_id}\s*['\"]?\s*:\s*\[)"
        
        if re.search(pattern, index_content, re.IGNORECASE):
            # æ£€æŸ¥æ˜¯å¦é‡å¤æ’å…¥
            if f"'{title}'" in index_content or f'"{title}"' in index_content:
                print(f"âš ï¸ é¦–é¡µåˆ—è¡¨ä¸­å·²å­˜åœ¨ã€Š{title}ã€‹ï¼Œè·³è¿‡æ’å…¥ã€‚")
            else:
                # ç”Ÿæˆæ–‡ç« å”¯ä¸€ ID
                article_id = f"art_{datetime.now().strftime('%H%M%S')}{random.randint(100, 999)}"
                new_entry = f"{{ id: '{article_id}', title: '{title}', filePath: '{md_path_web}', date: '{date_str}' }},"
                
                # åœ¨åŒ¹é…åˆ°çš„ [ åé¢ç›´æ¥æ¢è¡Œæ’å…¥æ–°å†…å®¹
                index_content = re.sub(pattern, f"\\1\n                {new_entry}", index_content, flags=re.IGNORECASE)
                
                index_content = re.sub(pattern, rf"\1\n            {new_entry}", index_content)
                with open(INDEX_FILE, 'w', encoding='utf-8') as f:
                    f.write(index_content)
                print(f"âœ… æˆåŠŸï¼šæ–‡ç« å·²åŒæ­¥åˆ° index.html çš„ {category_id} åˆ†ç±»ã€‚")
        else:
            print(f"âŒ åŒ¹é…å¤±è´¥ï¼šæœªåœ¨ index.html ä¸­æ‰¾åˆ°åˆ†ç±»æ ‡è¯† '{category_id}': [")
            print("è¯·æ£€æŸ¥ index.html é‡Œçš„åˆ†ç±» ID æ‹¼å†™æ˜¯å¦ä¸æ¬è¿é€‰æ‹©çš„ä¸€è‡´ã€‚")
                print(f"âœ… å·²å°†ã€Š{title}ã€‹æˆåŠŸåŒæ­¥è‡³ index.html çš„ {category_id} åˆ†ç±»")
            else:
                print(f"âŒ é”™è¯¯ï¼šåœ¨ index.html ä¸­æœªæ‰¾åˆ°åˆ†ç±»æ ‡è¯† '{category_id}'ï¼Œè¯·æ£€æŸ¥ ID æ˜¯å¦å®Œå…¨ä¸€è‡´")

    except Exception as e:
        print(f"âŒ è¿è¡Œä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"ğŸ’¥ è¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
if __name__ == \"__main__\":
    download_and_sync()
