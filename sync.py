import requests
from bs4 import BeautifulSoup
import os
import re
import sys
from datetime import datetime

# --- é…ç½® ---
BASE_DIR = "articles" 
IMAGE_SUBDIR = "images" 
VIDEO_SUBDIR = "videos" 
INDEX_FILE = "index.html" 

def download_and_sync():
    print("=== å“²å­¦å›­å…¨è‡ªåŠ¨æ›´æ–°ï¼šå¢å¼ºå†…å®¹æå–ç‰ˆ ===")
    
    if len(sys.argv) > 1 and sys.argv[1].strip():
        url = sys.argv[1].strip()
        category_id = sys.argv[2].strip() if len(sys.argv) > 2 else "laochan-column"
        print(f"ğŸ”— å¤„ç†é“¾æ¥: {url}")
    else:
        print("âŒ é”™è¯¯ï¼šæœªæ¥æ”¶åˆ°æ–‡ç« é“¾æ¥ã€‚")
        return

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    try:
        res = requests.get(url, headers=headers)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # 1. æŠ“å–æ ‡é¢˜
        title_tag = soup.find('h1', class_='rich_media_title') or soup.find('h1')
        title = title_tag.get_text().strip() if title_tag else "æœªå‘½åæ–‡ç« "
        safe_title = re.sub(r'[\\/:*?\"<>|]', '', title)
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # 2. å®šä½æ­£æ–‡ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„å¾®ä¿¡æ­£æ–‡ IDï¼‰
        # å¾®ä¿¡æ–‡ç« æ­£æ–‡é€šå¸¸åœ¨ js_content ä¸­ï¼Œä½†ä¹Ÿå¯èƒ½åœ¨å…¶ä»–åœ°æ–¹
        main_area = soup.find('div', id='js_content') or \
                    soup.find('div', class_='rich_media_content') or \
                    soup.find('div', id='img-content')

        if not main_area:
            print("âŒ æŠ“å–å¤±è´¥ï¼šæ‰¾ä¸åˆ°æ–‡ç« æ­£æ–‡åŒºåŸŸã€‚")
            return

        # åˆ›å»ºèµ„æºæ–‡ä»¶å¤¹
        os.makedirs(os.path.join(BASE_DIR, IMAGE_SUBDIR, safe_title), exist_ok=True)
        os.makedirs(os.path.join(BASE_DIR, VIDEO_SUBDIR, safe_title), exist_ok=True)

        md_content = f"# {title}\n\n---\n\n"
        img_count = 0
        
        # 3. éå†æå–å†…å®¹
        # ä½¿ç”¨ recursive=True æ·±åº¦æŸ¥æ‰¾æ‰€æœ‰æ®µè½å’Œå…ƒç´ 
        for element in main_area.find_all(['p', 'section', 'img'], recursive=True):
            # å¤„ç†å›¾ç‰‡
            if element.name == 'img':
                src = element.get('data-src') or element.get('src')
                if src and src.startswith('http'):
                    try:
                        img_res = requests.get(src, timeout=10)
                        if len(img_res.content) > 5000:
                            img_count += 1
                            img_name = f"{img_count}.jpg"
                            with open(os.path.join(BASE_DIR, IMAGE_SUBDIR, safe_title, img_name), 'wb') as f:
                                f.write(img_res.content)
                            md_content += f"![å›¾ç‰‡](articles/images/{safe_title}/{img_name})\n\n"
                    except: pass
                continue

            # å¤„ç†è§†é¢‘æ ‡è®° (æ¢æµ‹è§†é¢‘ç»„ä»¶)
            html_str = str(element)
            if 'finder_video_card' in html_str or 'video' in html_str:
                if 'video.mp4' not in md_content: # é¿å…é‡å¤æ ‡è®°
                    video_rel_path = f"articles/videos/{safe_title}/video.mp4"
                    md_content += f'\n<div style="text-align:center;"><video src="{video_rel_path}" controls style="max-width:100%"></video></div>\n\n'

            # æå–æ–‡æœ¬å†…å®¹
            text = element.get_text(strip=True)
            if text and not element.find('img'):
                # è¿‡æ»¤æ‰ä¸€äº›å¾®ä¿¡è‡ªå¸¦çš„å†—ä½™æç¤º
                if "æ‰«æäºŒç»´ç " not in text and "é˜…è¯»åŸæ–‡" not in text:
                    md_content += f"{text}\n\n"

        # 4. ä¿å­˜æ–‡ä»¶ä¸æ›´æ–°ç´¢å¼•
        md_file_path = os.path.join(BASE_DIR, category_id, f"{safe_title}.md")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"âœ… å†…å®¹å·²å†™å…¥: {md_file_path}")

        # æ›´æ–° index.html (é€»è¾‘ä¿æŒä¸å˜)
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            content = f.read()

        if f"'{title}'" not in content and f'"{title}"' not in content:
            md_path = f"articles/{category_id}/{safe_title}.md"
            new_entry = f"{{ id: 'art_{datetime.now().strftime('%M%S')}', title: '{title}', filePath: '{md_path}', date: '{date_str}' }},"
            pattern = rf"(['\"]?{category_id}['\"]?\s*:\s*\[)"
            if re.search(pattern, content):
                new_content = re.sub(pattern, f"\\1\n                {new_entry}", content)
                with open(INDEX_FILE, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                print(f"âœ… é¦–é¡µç´¢å¼•å·²æ›´æ–°ã€‚")

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    download_and_sync()
