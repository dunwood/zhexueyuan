import requests
from bs4 import BeautifulSoup
import os
import re
import sys
import random
from datetime import datetime

# --- åŸºç¡€é…ç½® ---
BASE_DIR = "articles" 
IMAGE_SUBDIR = "images" 
INDEX_FILE = "index.html" 

def download_and_sync():
    print("=== å“²å­¦å›­å…¨è‡ªåŠ¨æ›´æ–°ï¼šæ’ç‰ˆ+é»‘ä½“+é«˜å…¼å®¹ç‰ˆ ===")
    
    # 1. è·å–å‚æ•°
    if len(sys.argv) > 1 and sys.argv[1].strip():
        url = sys.argv[1].strip()
        category_id = sys.argv[2].strip() if len(sys.argv) > 2 else "laochan-column"
        print(f"ğŸ”— å¤„ç†é“¾æ¥: {url}")
        print(f"ğŸ“‚ ç›®æ ‡åˆ†ç±»: {category_id}")
    else:
        print("âŒ é”™è¯¯ï¼šæœªæ¥æ”¶åˆ°æœ‰æ•ˆçš„æ–‡ç« é“¾æ¥")
        return

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36'
    }

    try:
        res = requests.get(url, headers=headers)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # --- 2. è§£æå†…å®¹ ---
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ ‡é¢˜æ ‡ç­¾
        title_tag = soup.find('h1', class_='rich_media_title') or soup.find('h1', id='activity-name')
        if not title_tag:
            print("âŒ æ— æ³•è§£ææ–‡ç« æ ‡é¢˜ï¼Œè·³è¿‡")
            return
        title = title_tag.get_text(strip=True)
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # æ ¸å¿ƒï¼šå…¼å®¹å¤šç§æ­£æ–‡ ID
        content_area = soup.find('div', id='js_content') or soup.find('div', class_='rich_media_content')
        if not content_area:
            print("âŒ æ— æ³•è·å–æ–‡ç« æ­£æ–‡å†…å®¹")
            return

        # --- 3. å‡†å¤‡æ–‡ä»¶å¤¹ ---
        category_path = category_id.replace('/', os.sep)
        md_file_dir = os.path.join(BASE_DIR, category_path)
        img_dir = os.path.join(md_file_dir, IMAGE_SUBDIR)
        
        if not os.path.exists(img_dir):
            os.makedirs(img_dir, exist_ok=True)

        # --- 4. æ ¸å¿ƒï¼šæ¸…æ´—é€»è¾‘ ---
        # é¢„å¤„ç†ï¼šåˆ é™¤ä¸éœ€è¦çš„å¹²æ‰°
        for s in content_area(['script', 'style', 'noscript', 'iframe']):
            s.decompose()

        lines = []
        # éå†æ‰€æœ‰æ®µè½ã€æ ‡é¢˜å’Œå›¾ç‰‡
        for elem in content_area.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'img', 'section']):
            # A. å¤„ç†å›¾ç‰‡
            if elem.name == 'img':
                src = elem.get('data-src') or elem.get('src')
                if src:
                    img_count = len([f for f in os.listdir(img_dir) if f.startswith('img_')]) + 1
                    img_name = f"img_{img_count}.jpg"
                    img_path = os.path.join(img_dir, img_name)
                    try:
                        img_res = requests.get(src, headers=headers, timeout=10)
                        with open(img_path, 'wb') as f:
                            f.write(img_res.content)
                        lines.append(f"![å›¾ç‰‡]({IMAGE_SUBDIR}/{img_name})")
                    except:
                        pass
                continue

            # B. å¤„ç†é»‘ä½“å­— (ä¿ç•™ strong/b å¹¶è½¬ä¸º Markdown)
            for bold in elem.find_all(['strong', 'b']):
                b_text = bold.get_text(strip=True)
                if b_text:
                    bold.replace_with(f" **{b_text}** ")

            # C. å¤„ç†æ–‡æœ¬
            # å»æ‰æ ‡ç­¾è‡ªå¸¦çš„æ‰€æœ‰æ ·å¼å±æ€§ï¼Œé˜²æ­¢å½±å“æ˜¾ç¤º
            elem.attrs = {}
            text = elem.get_text(strip=True)
            if not text:
                continue
            
            # è§£å†³å¥å­æ–­å¼€ï¼šæŠ¹æ‰æ®µè½å†…éƒ¨ç¡¬æ¢è¡Œ
            clean_text = "".join(text.splitlines())
            
            # Markdown æ ¼å¼åˆ†é…
            if elem.name.startswith('h'):
                lines.append(f"### {clean_text}")
            else:
                lines.append(clean_text)

        # è§£å†³è¿æˆä¸€ç‰‡ï¼šæ®µè½é—´åŒæ¢è¡Œ
        content_body = "\n\n".join(lines)
        md_content = f"# {title}\n\n> å‘å¸ƒæ—¥æœŸ: {date_str}\n\n{content_body}"

        # --- 5. ä¿å­˜ Markdown ---
        safe_title = re.sub(r'[\\/:*?"<>|]', '_', title)
        md_file_path = os.path.join(md_file_dir, f"{safe_title}.md")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"ğŸ“ Markdown å·²ç”Ÿæˆ: {md_file_path}")

        # --- 6. åŒæ­¥æ›´æ–° index.html ---
        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            index_content = f.read()

        # æŸ¥é‡é€»è¾‘
        if f"'{title}'" in index_content or f'"{title}"' in index_content:
            print(f"âš ï¸ é¦–é¡µåˆ—è¡¨ä¸­å·²å­˜åœ¨è¯¥æ–‡ç« ï¼Œè·³è¿‡æ’å…¥ã€‚")
        else:
            md_path_web = f"articles/{category_id}/{safe_title}.md"
            article_id = f"art_{datetime.now().strftime('%H%M%S')}{random.randint(100, 999)}"
            new_entry = f"{{ id: '{article_id}', title: '{title}', filePath: '{md_path_web}', date: '{date_str}' }},"
            
            pattern = rf"(['\"]{re.escape(category_id)}['\"]\s*:\s*\[)"
            if re.search(pattern, index_content):
                index_content = re.sub(pattern, rf"\1\n            {new_entry}", index_content)
                with open(INDEX_FILE, 'w', encoding='utf-8') as f:
                    f.write(index_content)
                print(f"âœ… å·²æˆåŠŸåŒæ­¥è‡³ index.html")
            else:
                print(f"âŒ é”™è¯¯ï¼šåœ¨ index.html ä¸­æœªæ‰¾åˆ°åˆ†ç±»æ ‡è¯† '{category_id}'")

    except Exception as e:
        print(f"ğŸ’¥ è¿è¡Œå‡ºé”™: {e}")

if __name__ == "__main__":
    download_and_sync()
