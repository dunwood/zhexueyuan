import requests
from bs4 import BeautifulSoup
import os
import re
import sys  # ã€æ–°å¢ï¼šç”¨äºè¯»å–å¤–éƒ¨å‚æ•°ã€‘
from datetime import datetime

# --- è§„èŒƒåŒ–è·¯å¾„é…ç½® ---
BASE_DIR = "articles" 
IMAGE_SUBDIR = "images" 
VIDEO_SUBDIR = "videos" 
INDEX_FILE = "index.html" 

def download_and_sync():
    print("=== å“²å­¦å›­ä¸€é”®åŒæ­¥ (GitHub Actions äº‘ç«¯é€‚é…ç‰ˆ) ===")
    
    # --- ä¿®æ”¹è¾“å…¥é€»è¾‘ï¼šä¼˜å…ˆè¯»å–å¤–éƒ¨å‚æ•°ï¼Œæ²¡æœ‰åˆ™æ‰‹åŠ¨è¾“å…¥ ---
    if len(sys.argv) > 1:
        url = sys.argv[1].strip()
        category_id = sys.argv[2].strip() if len(sys.argv) > 2 else "laochan-column"
        print(f"ğŸ”— æ­£åœ¨å¤„ç†é“¾æ¥: {url}")
        print(f"ğŸ“ ç›®æ ‡åˆ†ç±»: {category_id}")
    else:
        url = input("è¯·è¾“å…¥å¾®ä¿¡æ–‡ç« é“¾æ¥: ").strip()
        if not url: return
        print("\nè¯·é€‰æ‹©æ–‡ç« åˆ†ç±»: [1]è€è‰ä¸“æ  [2]å¤å¸Œè…Š [3]å½¢è€Œä¸Šå­¦ ...")
        cat_map = {"1":"laochan-column", "2":"ancient-greek", "3":"metaphysics", "4":"ethics", "5":"epistemology", "6":"logic", "7":"aesthetics", "8":"math-science-philosophy"}
        category_id = cat_map.get(input("è¯·è¾“å…¥ç¼–å· (é»˜è®¤'1'): ").strip(), "laochan-column")

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    try:
        res = requests.get(url, headers=headers)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # 1. æ ‡é¢˜å¤„ç†
        title_tag = soup.find('h1', class_='rich_media_title')
        title = title_tag.get_text().strip() if title_tag else "æœªå‘½å"
        safe_title = re.sub(r'[\\/:*?\"<>|]', '', title)
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # åˆ›å»ºæ–‡ä»¶å¤¹å‘ä½
        local_img_dir = os.path.join(BASE_DIR, IMAGE_SUBDIR, safe_title)
        os.makedirs(local_img_dir, exist_ok=True)
        local_video_dir = os.path.join(BASE_DIR, VIDEO_SUBDIR, safe_title)
        os.makedirs(local_video_dir, exist_ok=True)

        # 2. æ­£æ–‡æŠ“å– (ä¿ç•™åŸå§‹é€»è¾‘)
        main_area = soup.find('div', id='js_content')
        md_content = f"# {title}\n\n---\n\n"
        img_count = 0
        
        for element in main_area.children:
            if isinstance(element, str):
                if element.strip(): md_content += f"{element.strip()}\n\n"
                continue
            if element.find_all('img') or element.name == 'img':
                imgs = element.find_all('img') if element.name != 'img' else [element]
                for img in imgs:
                    src = img.get('data-src') or img.get('src')
                    if src and 'wx_fmt=gif' not in src:
                        try:
                            img_res = requests.get(src, headers={'Referer': 'https://mp.weixin.qq.com/'}, timeout=10)
                            if len(img_res.content) > 5000:
                                img_count += 1
                                img_name = f"{img_count}.jpg"
                                with open(os.path.join(local_img_dir, img_name), 'wb') as f: f.write(img_res.content)
                                md_content += f"![å›¾ç‰‡](articles/images/{safe_title}/{img_name})\n\n"
                        except: continue
            
            # å¤„ç†è§†é¢‘åµŒå…¥æ ‡ç­¾
            html_str = str(element)
            if 'finder_video_card' in html_str or element.find('iframe') or 'video' in html_str:
                video_rel_path = f"articles/videos/{safe_title}/video.mp4"
                video_tag = f'\n<div style="text-align:center;"><video src="{video_rel_path}" controls style="max-width:100%"></video></div>\n'
                if video_tag not in md_content: md_content += video_tag

            text = element.get_text(strip=True)
            if text and not element.find('img'): md_content += f"{text}\n\n"

        # 3. ä¿å­˜ MD æ–‡ä»¶
        md_file_dir = os.path.join(BASE_DIR, category_id)
        os.makedirs(md_file_dir, exist_ok=True)
        md_file_name = f"{safe_title}.md"
        with open(os.path.join(md_file_dir, md_file_name), 'w', encoding='utf-8') as f: f.write(md_content)

        # 4. åŒæ­¥ index.html
        with open(INDEX_FILE, 'r', encoding='utf-8') as f: index_content = f.read()
        if f"'{title}'" in index_content or f'"{title}"' in index_content:
            print(f"âš ï¸ é¦–é¡µå·²å­˜åœ¨ã€Š{title}ã€‹")
        else:
            md_path_for_index = f"articles/{category_id}/{md_file_name}"
            article_id = f"art_{datetime.now().strftime('%H%M%S')}"
            new_entry = f"{{ id: '{article_id}', title: '{title}', filePath: '{md_path_for_index}', date: '{date_str}' }},"
            pattern = rf"(['\"]{category_id}['\"]:\s*\[)"
            index_content = re.sub(pattern, f"\\1\n                {new_entry}", index_content)
            with open(INDEX_FILE, 'w', encoding='utf-8') as f: f.write(index_content)
            print(f"âœ… æ›´æ–°æˆåŠŸã€‚")

    except Exception as e:
        print(f"âŒ å‡ºé”™: {e}")

if __name__ == "__main__":
    download_and_sync()