import requests
from bs4 import BeautifulSoup
import os
import re
import sys
from datetime import datetime

# --- åŸºç¡€é…ç½® ---
BASE_DIR = "articles" 
IMAGE_SUBDIR = "images" 
VIDEO_SUBDIR = "videos" 
INDEX_FILE = "index.html" 

def download_and_sync():
    print("=== å“²å­¦å›­å…¨è‡ªåŠ¨åŒæ­¥ç³»ç»Ÿ (å¢å¼ºç‰ˆ) ===")
    
    # 1. è¯»å–å‚æ•°ï¼šä¼˜å…ˆè¯»å– GitHub Actions ä¼ æ¥çš„åˆ†ç±»å’Œé“¾æ¥
    if len(sys.argv) > 1 and sys.argv[1].strip():
        url = sys.argv[1].strip()
        # å¦‚æœä¸‹æ‹‰èœå•ä¼ æ¥äº†åˆ†ç±» IDï¼Œå°±ç”¨å®ƒï¼›å¦åˆ™é»˜è®¤è€è‰ä¸“æ 
        category_id = sys.argv[2].strip() if len(sys.argv) > 2 else "laochan-column"
        print(f"ğŸ”— æ­£åœ¨å¤„ç†é“¾æ¥: {url}")
        print(f"ğŸ“‚ ç›®æ ‡åˆ†ç±» ID: {category_id}")
    else:
        print("âŒ é”™è¯¯ï¼šæœªæ¥æ”¶åˆ°æœ‰æ•ˆçš„æ–‡ç« é“¾æ¥ã€‚")
        return

    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

    try:
        res = requests.get(url, headers=headers)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')

        # --- æŠ“å–æ ‡é¢˜ ---
        title_tag = soup.find('h1', class_='rich_media_title')
        title = title_tag.get_text().strip() if title_tag else "æœªå‘½åæ–‡ç« "
        safe_title = re.sub(r'[\\/:*?\"<>|]', '', title)
        date_str = datetime.now().strftime('%Y-%m-%d')
        
        # --- åˆ›å»ºæ–‡ä»¶å¤¹å‘ä½ ---
        os.makedirs(os.path.join(BASE_DIR, IMAGE_SUBDIR, safe_title), exist_ok=True)
        os.makedirs(os.path.join(BASE_DIR, VIDEO_SUBDIR, safe_title), exist_ok=True)
        print(f"ğŸ“ æ–‡ä»¶å¤¹å·²å°±ç»ª: {safe_title}")

        # --- æ­£æ–‡å¤„ç†é€»è¾‘ ---
        main_area = soup.find('div', id='js_content')
        md_content = f"# {title}\n\n---\n\n"
        img_count = 0
        
        for element in main_area.children:
            if isinstance(element, str):
                if element.strip(): md_content += f"{element.strip()}\n\n"
                continue
            
            # å¤„ç†å›¾ç‰‡
            if element.find_all('img') or element.name == 'img':
                imgs = element.find_all('img') if element.name != 'img' else [element]
                for img in imgs:
                    src = img.get('data-src') or img.get('src')
                    if src:
                        try:
                            img_res = requests.get(src, timeout=10)
                            if len(img_res.content) > 5000:
                                img_count += 1
                                img_name = f"{img_count}.jpg"
                                img_path = os.path.join(BASE_DIR, IMAGE_SUBDIR, safe_title, img_name)
                                with open(img_path, 'wb') as f: f.write(img_res.content)
                                md_content += f"![å›¾ç‰‡](articles/images/{safe_title}/{img_name})\n\n"
                        except: continue
            
            # æ ‡è®°è§†é¢‘ä½ç½®
            html_str = str(element)
            if 'finder_video_card' in html_str or element.find('iframe') or 'video' in html_str:
                video_rel_path = f"articles/videos/{safe_title}/video.mp4"
                md_content += f'\n<div style="text-align:center;"><video src="{video_rel_path}" controls style="max-width:100%"></video></div>\n'

            # æå–æ–‡æœ¬
            text = element.get_text(strip=True)
            if text and not element.find('img'): md_content += f"{text}\n\n"

        # --- ä¿å­˜ Markdown æ–‡ä»¶ ---
        md_file_dir = os.path.join(BASE_DIR, category_id)
        os.makedirs(md_file_dir, exist_ok=True)
        md_file_path = os.path.join(md_file_dir, f"{safe_title}.md")
        with open(md_file_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        print(f"ğŸ“ Markdown å·²ç”Ÿæˆ: {md_file_path}")

        # --- æ ¸å¿ƒï¼šåŒæ­¥æ›´æ–° index.html ---
        if not os.path.exists(INDEX_FILE):
            print(f"âŒ æ‰¾ä¸åˆ° {INDEX_FILE}")
            return

        with open(INDEX_FILE, 'r', encoding='utf-8') as f:
            index_content = f.read()

        # æ£€æŸ¥æ˜¯å¦é‡å¤
        if f"'{title}'" in index_content or f'"{title}"' in index_content:
            print(f"âš ï¸ é¦–é¡µåˆ—è¡¨ä¸­å·²å­˜åœ¨ã€Š{title}ã€‹ï¼Œè·³è¿‡æ’å…¥ã€‚")
        else:
            md_path_for_web = f"articles/{category_id}/{safe_title}.md"
            article_id = f"art_{datetime.now().strftime('%H%M%S')}"
            new_entry = f"{{ id: '{article_id}', title: '{title}', filePath: '{md_path_for_web}', date: '{date_str}' }},"
            
            # æ­£åˆ™è¡¨è¾¾å¼è¯´æ˜ï¼šåŒ¹é… "åˆ†ç±»å": [ æˆ– 'åˆ†ç±»å': [ï¼Œä¸è®ºç©ºæ ¼å¤šå°‘
            pattern = rf"(['\"]{category_id}['\"]\s*:\s*\[)"
            
            if re.search(pattern, index_content):
                # åœ¨æ‰¾åˆ°çš„æ ‡å¿—åé¢æ’å…¥æ–°æ¡ç›®
                index_content = re.sub(pattern, f"\\1\n                {new_entry}", index_content)
                with open(INDEX_FILE, 'w', encoding='utf-8') as f:
                    f.write(index_content)
                print(f"âœ… é¦–é¡µ index.html å·²æˆåŠŸæ›´æ–°ï¼Œæ–‡ç« å·²å½’ç±»è‡³: {category_id}")
            else:
                print(f"âŒ åŒ¹é…å¤±è´¥ï¼šåœ¨ index.html ä¸­æ²¡æ‰¾åˆ° '{category_id}': [ çš„æ ‡å¿—ã€‚")
                print("ğŸ’¡ è¯·æ£€æŸ¥ index.html é‡Œçš„åˆ†ç±» ID æ˜¯å¦ä¸ä¸‹æ‹‰èœå•ä¸­çš„ ID å®Œå…¨ä¸€è‡´ã€‚")

    except Exception as e:
        print(f"âŒ è¿è¡Œä¸­å‘ç”Ÿå´©æºƒ: {e}")

if __name__ == "__main__":
    download_and_sync()
