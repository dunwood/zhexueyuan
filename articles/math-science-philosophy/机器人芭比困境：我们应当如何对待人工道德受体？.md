# 机器人芭比困境：我们应当如何对待人工道德受体？

---

转自：哲学社

机器人芭比困境：

我们应当如何对待人工道德受体？

摩根·勒克 ¹，托马斯·蒙特菲奥里 ¹, ²，以及 克里斯托弗·巴特尔 ¹, ³

¹ 澳大利亚查尔斯特大学，² 澳大利亚麦考瑞大学，³ 美国阿巴拉契亚州立大学

人工道德受体（简称 AMPs）
是指那些被成功制造出来模仿道德受体、但本身并非道德受体的事物。它们的“人工性”体现在两个层面：一是它们由人类制造（人工制品），二是它们并非其所模仿对象的真实实例（拟态）。ChatGPT、性爱娃娃、社交机器人以及非玩家角色（NPC）都是人工道德受体的实例。随着这些技术模仿人类的精确度不断提高，关于我们应当如何对待它们的问题变得日益重要。我们探讨关于“游戏玩家困境”（一个涉及虚拟错误行为的难题）的研究是否能为解决这一问题提供有用的框架。

关键词：
人工错误行为；游戏玩家困境；数字伦理；社交机器人学；虚构的不道德行为；人机交互。

![图片](articles/images/机器人芭比困境：我们应当如何对待人工道德受体？/1.jpg)

I. 介绍人工错误行为

我们关注的是某些人工错误行为在道德上的准许性。人工错误行为是指那些针对人工道德受体实施的行为，如果这些行为是针对真实的道德受体（即道德行为的接收者）实施的，则会构成真实的错误行为 (Floridi 2021: 185)。人工道德受体（AMP）是指被成功制造出来模仿道德受体、但其本身并不属于道德受体的事物。AMP 的人工性在于：它们既是人类制造的（人工制品），又不是其所模仿对象的实际实例（拟态）。例如，芭比娃娃就是一个 AMP，因为它被制造出来的目的是模仿人类（一种道德受体），但它本身并不是人。因此，拔掉芭比娃娃的头将构成一种人工错误行为；因为如果这个娃娃是一个真实的人，拔掉其头部将是客观错误的。

通信作者：
摩根·勒克，moluck@csu.edu.au
© 作者 2025。
由牛津大学出版社代表苏格兰哲学协会和圣安德鲁斯大学出版。
许可声明：
这是一篇根据知识共享署名-非商业性使用许可协议分发的开放获取文章，在正确引用原文的前提下，允许在任何媒介中进行非商业性的重复使用、分发和复制。如需商业用途的重复使用，请联系 journals.permissions@oup.com。
下载信息：
由访客于 2026 年 1 月 24 日从牛津大学出版社官网下载。

许多人工错误行为似乎并不构成真实的错误行为。例如，拔掉芭比娃娃的头似乎并没错——因为我们知道那只是塑料。然而，某些人工错误行为似乎也确实是真实的错误行为。例如，一名连环杀手在黑暗的小巷里刺伤了一个人体模型，由于他误以为那是真人，因此他既犯下了人工错误行为（因为如果模型是真人，这将构成真实的错误行为），也犯下了真实的错误行为（因为他企图刺杀一个他认为的人）。

在这两种情况之间存在一个灰色地带，其中人工错误行为的错误性质较不明确。例如，想象一个由先进机器人技术和人工智能制造的芭比——机器人芭比。这个娃娃不仅在外观上，而且在行为上也更加像人。它可能会祈求怜悯，并在“痛苦”中哭泣。然而，我们并不会把机器人芭比误认为是人类——毕竟她仍然只有娃娃那么大。拔掉这样一个娃娃的头会是错误的吗？达林指出，“对机器人对象采取暴力行为，对我们许多人来说感觉是错误的，即使我们知道被‘虐待’的对象没有任何感知能力” (2016 : 12–3)。但是，尽管它可能感觉是错误的，它实际上是错误的吗？这就是我们的核心问题。

我们在此提出的问题虽然与关于机器人和人工智能道德地位的长期争论相关，但又有所区别。¹ 许多哲学家都在思考机器人是否应该、以及在什么时候应该被视为道德受体。这方面的文献非常广泛。简而言之，一些人认为只有当机器人具备适当的道德相关属性（如感知力、智能或自主行为）时，才应被授予道德受体地位 (Neely 2014)。另一些人则建议，我们无需试图确定机器人是否在本质上具备这些属性，而是可以将道德受体身份赋予那些在行为上等同 (Danaher 2020) 或在认知上等同 (Shevlin 2021) 于公认道德受体的机器人。最后，还有一些人认为，道德受体身份可能取决于我们与机器人的关系 (Gunkel 2012, 2023; Coeckelbergh 2014; Coeckelbergh and Gunkel 2014; Bryson 2018)。我们的目标并非提供关于道德受体身份的通用解释，而是探讨在一个直觉冲突案例中的表现，即：当我们确信该机器人不是道德受体时的情况。

注释：

¹ 参见 Gunkel (2012, 2023) 以及 Harris 和 Anthis (2021)，了解关于这一更广泛问题的优秀文献综述。

II. 什么是中度人工道德受体（Mid-AMPs）？

我们关注的是针对特定类型 AMP（如“机器人芭比”）实施的人工错误行为。这些 AMP 处于一个连续谱系的特定区域，该谱系标记了某物与道德受体的相似程度。AMP 和非 AMP 都可以放置在这个连续谱系上。谱系的一端是真实的道德受体，比如 2023 年电影《芭比》的主演玛格特·罗比（Margot Robbie）。另一端则是与道德受体相似度极低的实体，比如普通的芭比娃娃，甚至更远处的石头。

在这个谱系上，某些 AMP 会非常接近玛格特·罗比，其相似程度高到让我们将其误认为真人。想象一个极其先进的类人机器人，它与玛格特·罗比完全无法区分——我们可以称之为“玛格特·机器人比”（Margot Robo-tie）。这类事物跨越了连续谱系上的某个阈值，我们称之为
高相似度阈值
。跨越这一阈值的事物具有极强的说服力，以至于我们相信它们就是道德受体。（或者，它们至少具有足够的说服力，使得将它们视为道德受体是审慎的做法；因为如果高相似度阈值的界限足够模糊，考虑到后果严重性，最好采用“预防原则”，宁可错在谨慎一边）。而那些未跨越该阈值的事物则缺乏这种高度的说服力。

跨越这一阈值具有道德意义。例如，在黑暗小巷中刺伤人体模型并误以为其是真人的连环杀手，其行为远比刺伤一个已知是模型的人要恶劣得多。科克尔伯格（Coeckelbergh）提到了这种阈值的道德重要性，他指出：

……我们应该认真对待外表的伦理意义，不要只关注机器人的主体性和“心灵”问题，而应关注我们如何感知机器人 (2009 : 220–1)。

弗里德曼（Friedman, 2020）将跨越这一阈值的 AMP 称为“感知道德受体”，但为了保持一致性，我们将其称为
高相似度人工道德受体
，简称
Hi-AMPs
。

沿着谱系的另一个方向，是那些与道德受体相似度较低的事物。在这个方向上存在第二个阈值——
低相似度阈值
——它标记了那些相似度不足以让我们轻易将其想象为道德受体的事物。换句话说，它们对于被视为道德受体具有足够的“抵抗力”。请注意，我们并不是说无法想象此类事物是道德受体，只是说这样做并不容易。我们将低于此阈值的 AMP 称为
低相似度人工道德受体
，简称
Low-AMPs
。普通的芭比娃娃就是 Low-AMP 的一个例子。

处于低相似度阈值和高相似度阈值之间的，是那些相似度足以让我们轻易将其想象为道德受体、但又不足以让我们信以为真的事物。也就是说，它们在
想象引导性
上足以让人将其视为道德受体——某物与道德受体越相似，就越能引导人们进入这种虚构。例如，一个写实的性爱娃娃之所以能吸引某些人，正是因为很容易将其想象成真人。我们将处于这两个阈值之间的区域称为
中度相似人工道德受体
，简称
Mid-AMPs
。“机器人芭比”就是 Mid-AMP 的一个例子。

![图片](articles/images/机器人芭比困境：我们应当如何对待人工道德受体？/2.jpg)

为了帮助直观理解这些不同类型的 AMP，请参考图 1。

区分了这些类型的 AMP 后，我们的核心问题可以重新表述如下：

针对 Mid-AMP 实施的人工错误行为，在道德上是否等同于针对 Low-AMP 实施的相同人工错误行为？

例如，如果我们正在考虑拆掉芭比娃娃的头是否被允许，相关的比较性问题就是：在类似条件下，拆掉“机器人芭比”的头是否也被允许。

在继续讨论之前，有三点补充说明。

首先，Mid-AMP 兼具
客观
和
主观
成分。客观上，它们是具有引发某种状态倾向的人造物品（非自然产生的事物）。但这种状态是主观的——即能够轻易将该 AMP 想象为道德受体的状态。因此，什么构成 Mid-AMP 可能因人而异。例如，了解大型语言模型的工作原理可能会让人更难将 ChatGPT 想象成道德受体，而不了解这些的人则更容易产生这种联想。所以，同一个物品对一个人（如儿童）来说可能是 Mid-AMP，但对另一个人（如人工智能工程师）则不是。

其次，某物模仿道德受体的方式有多种。例如，人体模型在外观上更像人，但在行为上较差。相反，ChatGPT 在语言行为上更像人，但在外观上则不然。
情境
也很重要。例如，人体模型在黑暗小巷中可能看起来非常像人，但在光线充足的展厅里则逊色许多。因此，什么构成 Mid-AMP 还取决于具体情境。在本文的研究中，如果一个物品在任何方面或情境下，其相似程度足以让人轻易将其想象为道德受体（但又不至于信以为真），那么它对该个体而言就符合 Mid-AMP 的定义。

第三，有人可能会认为，高相似度人工道德受体（Hi-AMP）甚至中度人工道德受体（Mid-AMP）的概念本身在逻辑上是不连贯的。例如，埃尔顿（Elton）主张素食者不应对电子游戏中的非玩家角色（NPC）实施虚拟错误行为，因为它们“表现出各种栩栩如生的属性，且……显示出某些认知能力” (2000 : 24)。这些属性使它们成为了“生物”，从而相应地成为了道德受体。类似地，达纳赫（Danaher, 2020）辩称，赋予某物伦理地位的标准在于其行为是否像一个道德受体：

如果一个机器人在表现上大致等同于另一个被公认具有重大道德地位的实体，那么赋予该机器人同样的地位是正当且合理的。
机器人可以在表现上大致等同于其他被公认具有重大道德地位的实体。
因此：
赋予机器人重大的道德地位可以是正当且合理的。
(Danaher 2020 : 2025) ²

考虑到我们（错误地）将 Hi-AMP 视为道德受体，它们可能同时满足达纳赫和埃尔顿的标准；而且，由于 Mid-AMP 很容易被想象成道德受体，它们也可能满足这些标准。在这种情况下，根据达纳赫和埃尔顿的观点，这些所谓的 AMP 可能其实是
真实的
道德受体，因此根本不是“人工”道德受体（因为根据定义，AMP 绝非真实的道德受体）。这就是所谓的“不连贯性”所在。

为了回应这一反对意见，我们必须坚持认为：一件事物
呈现为
道德受体但实际上
并非
道德受体是可能的。虽然我们在此不对此展开辩论，但可以迅速提出两点相关的观察：首先，虽然出于预防性原因，将高度模仿道德受体的事物视为道德受体是审慎的，但这种对待并不必然产生“它是道德受体”的信念，也不会使其真的变成道德受体。其次，由于我们是在比较 Low-AMP 与 Mid-AMP，即使 Hi-AMP 的概念被证明是不连贯的，我们的比较研究依然可以成立。

III. 从“游戏玩家困境”到“机器人芭比困境”

已经引起一定关注的一类人工错误行为是
虚拟错误行为
。在这里，虚拟错误行为是指针对虚拟道德受体实施的行为，如果该受体是真实的道德受体，该行为将构成真实的错误行为。虚拟道德受体是道德受体的数字表现形式，例如电子游戏《侠盗猎车手 5》（GTA5）中的非玩家角色（NPC）。

注释：

² 关于对达纳赫立场的批评，请参见 Müller (2021)、Musiał (2023) 以及 Smids (2020)。

虚拟错误行为的可准许性是“游戏玩家困境”（Gamer’s Dilemma, Luck 2009）的核心。这是一个关于我们在玩电子游戏时可能执行的两种不同虚拟行为之准许性的难题。我们的目标是借鉴解决“游戏玩家困境”的不同方法，来帮助我们思考人工错误行为的准许性。

“游戏玩家困境”涉及的第一类虚拟行为是“虚拟谋杀”，即电子游戏玩家在虚拟世界中故意指使自己的角色谋杀另一人。例如，在玩《侠盗猎车手 5》（GTA5）时，玩家在某些情况下故意撞死无辜行人（即 NPC），如果这些行人是真实的，该行为将构成真实的谋杀。虚拟谋杀在许多极受欢迎的电子游戏系列中司空见惯，如《侠盗猎车手》、《荒野大镖客》和《上古卷轴》。然而，这在道德上是允许的吗？也许是允许的，因为并没有人真正被谋杀——这“仅仅是个游戏”。

然而，同样的话也可以用于一种不太常见的行为——“虚拟猥亵儿童”。即玩家故意指使自己的成年角色猥亵一名儿童 NPC。同样，没有儿童真正受到侵害，这依然“仅仅是个游戏”。然而，我们似乎不太愿意承认这种虚拟错误行为的准许性。但事实证明，很难在这两种行为之间找到相关的区别。这个难题可以以悖论的形式呈现：

前提 1 (P1)
—— 虚拟谋杀是准许的。
前提 2 (P2)
—— 在准许性方面，虚拟谋杀与虚拟猥亵儿童之间没有相关区别。
前提 3 (P3)
—— 虚拟猥亵儿童是不准许的。

这就是“游戏玩家困境”的一般形式；一组共同矛盾命题，但每一个看起来都是正确的（或至少看起来不明显错误）。

目前出现了三种应对“游戏玩家困境”的主要策略：

消除策略
（反对 P1 或 P3 真实性的论证）、

解决策略
（反对 P2 真实性的论证）以及

抵制策略
（反对支持 P1 或 P3 直觉的论证）。

消除策略
（Dissolving strategies）
由那些认为“虚拟谋杀是不准许的”或“虚拟猥亵儿童是准许的”人所采用（但不能两者同时成立，否则会创造出一个同样令人困惑的“反向游戏玩家困境”）(Ali 2015; Luck 2018; Ramirez 2020)。

解决策略
（Resolving strategies）
由那些认为“虚拟谋杀是准许的、但虚拟猥亵儿童是不准许的”人所采用，他们试图寻找两者之间的相关区别 (Bartel 2012, 2020; Luck and Ellerby 2013; Patridge 2013; Young 2016; Nader 2020; Kjeldgaard-Christiansen 2020; 以及 Milne and Ivankovic 2021)。

抵制策略
（Resisting strategies）
由那些质疑支持 P1 或 P3 的直觉的人所采用（而不是直接论证 P1 或 P3 的真假）(Montefiore and Formosa 2022, 2023a, 2023b; Formosa et al. 2023; Ekdahl 2023; Luck 2023, 2024)。

我们的目标是借鉴这些策略，来帮助我们思考人工错误行为的准许性。

“游戏玩家困境”随后被扩大，以适用于更广泛意义上的虚构错误行为 (Luck 2022; Montefiore and Formosa 2023a; Montefiore et al. 2024)。虚构错误行为是指针对一个虚构道德受体实施的行为，如果该受体是一个真实的道德受体，那么该行为将构成真实的错误行为。这种被扩大的困境具有以下架构：

P1
—— 在 X 情况下，参与虚构错误行为 A 是准许的。
P2
—— 在准许性方面，X 情况下参与虚构错误行为 A 与 X 情况下参与虚构错误行为 B 之间没有相关区别。
P3
—— 在 X 情况下，参与虚构错误行为 B 是不准许的。

这一架构将困境限制在“X 类情况”中，使我们能够撇开那些命题可能显得虚假的边缘案例 (Bartel 2012; Ali 2015; Luck 2022; Montefiore and Formosa 2022)。例如，考虑一名为政府分级委员会工作的审核官员，他为了推荐合适的等级而玩电子游戏。在这种语境下，实施虚拟猥亵儿童的行为似乎是允许的 (Luck 2022: 1292)。但这属于非典型的游戏行为，因此不属于“X 类情况”。

该架构能够包含“游戏玩家困境”的不同变体。例如，为什么在电子游戏《上古卷轴：天际》（Skyrim）中抢劫店主是允许的（错误行为 A），但对他大声辱骂恐同言论却是不允许的（错误行为 B）。它还涵盖了参与此类虚构错误行为的不同方式。例如，为什么跟着披头士乐队的歌曲《麦克斯韦的银锤》（Maxwell’s Silver Hammer，内容关于一名连环杀手用重物击杀多人）愉快地合唱是允许的（错误行为 A），但一首愉快地描述猥亵儿童者行径的歌曲却是不允许的（错误行为 B）(Luck 2022: 1297)。

通过将架构扩大到虚构错误行为，我们将虚拟错误行为和人工错误行为整合到了同一个范畴之下。因为虚拟错误行为是人工错误行为的一个子类，因为虚拟道德受体（如 NPC）就是 AMP（人工道德受体）。也就是说，它们是为了模仿道德受体而制造的数字人工制品（但并非真正的受体）。同样地，人工错误行为也是虚构错误行为的一个子类，因为 AMP 就是虚构道德受体。也就是说，它们是具有假装成道德受体之属性的人工制品（但并非真正的受体）。鉴于此，请重新审视我们的核心问题：

针对 Mid-AMP 实施的人工错误行为，在道德上是否等同于针对 Low-AMP 实施的相同人工错误行为？

这个问题可以被视为一个符合更广泛困境架构的难题；如下所示：

P1
—— 在 X 情况下，人工谋杀一个 Low-AMP（普通芭比）是准许的。
P2
—— 在准许性方面，人工谋杀 Low-AMP 与人工谋杀 Mid-AMP 之间没有相关区别。
P3
—— 在 X 情况下，人工谋杀一个 Mid-AMP（机器人芭比）是不准许的。

为了帮助我们思考这个困境，让我们关注一个更具体的实例——
“机器人芭比困境”
。对我们许多人来说，P1 和 P3 在直觉上似乎都是正确的：

P1
—— 人工谋杀“芭比娃娃”是准许的。
P2
—— 在准许性方面，人工谋杀“芭比”与人工谋杀“机器人芭比”没有相关区别。
P3
—— 人工谋杀“机器人芭比”是不准许的。

这些人工错误行为依然被限制在 X 情况下，排除了人工谋杀的某些边缘实例。例如，我们撇开了一名技术人员为了维修而拆除机器人芭比头部的情况，因为这超出了我们关注的范围。同样地，我们排除了虐待 AMP 直接伤害到第三方的情况，例如一个孩子故意拔掉兄弟姐妹的芭比娃娃的头以使其伤心 (Coeckelbergh 2021)。这类人工谋杀案例可能是不准许的，尽管其原因不一定是由于我们讨论的核心所致。

还必须承认，我们对“机器人芭比困境”的直觉可能会受到机器人芭比代表女性这一事实的影响。因为针对女性暴力的悠久历史可能正当地作用于我们的道德直觉。然而，我们认为潜在的难题足够广泛，也包含了 AMP 不代表女性的情况。例如，如果我们考虑对待塑料玩具狗与仿真机器人狗的区别，这一难题可能依然成立。

幸运的是，已经有大量工作可以被借鉴来应对这一新困境。因为我们许多人都关注那些看似迥异的技术的伦理对待，如性爱机器人、非玩家角色（NPC）、人工智能（AI）和社交机器人。既然这些技术都是人工道德受体（AMP），我们可以利用这些研究工作来应对“机器人芭比困境”。

我们将把对这一困境的回应分为消除、解决和抵制策略，镜像参照那些用于“游戏玩家困境”的策略。首先，我们检验消除“机器人芭比困境”的方法。

IV. 消除机器人芭比困境

存在两种类型的消除策略。第一种是
非道德主义消除
（amoralist dissolution），其支持者认为人工谋杀芭比娃娃和机器人芭比都是准许的（即 P3 为假）。第二种是
道德主义消除
（moralist dissolution），其支持者认为人工谋杀芭比娃娃和机器人芭比都是不准许的（即 P1 为假）。接下来我们将考虑这两种消除方式。

IV.1 无道德主义者的消解方案

无道德主义者（amoralists）通常主张，对拟人化道德对象（AMPs，即 artificial moral patients，如性爱机器人等）实施人为的错误行为是允许的，因为它们并非道德受体（moral patients），因此可以被仅仅当作人工制品来对待。如果这一观点正确，那么前提 P3 就是错误的，从而消解了“机器芭比困境”（Robo-Barbie Dilemma）。 然而，接受无道德主义消解方案的代价可能相当高昂。例如，在“游戏者困境”（gamer’s dilemma）中，无道德主义消解方案认为，虚拟儿童性侵是允许的，理由是没有真实的儿童受到伤害。无论虚拟行为多么生动、残忍，无道德主义者都坚持其允许性，因为它仅仅是虚构的（参见 Bartel 2020：第2章的批评）。 现在让我们来看这类论证如何应用于“机器芭比困境”。

IV.1.1 康德的“无尊严”消解方案

在 Moen 和 Sterri（2018）关于允许使用儿童性爱机器人的论证基础上，Gordon 和 Nyholm（2022）提出了一种康德式的论证来支持其允许使用。根据这一论证，只有人才需要被以尊严对待，因为只有人才是具有理性和自主性的存在。因此，虽然人不应被当作手段来使用，但其他一切事物都可以被允许如此使用。因此，由于儿童性爱机器人不是人，它们可以被当作达到目的的手段来使用。正如 Bryson 所言：“机器人就是工具，在伦理领域与其他任何人工制品一样”（2010：73）。

这一思路表明，可以随心所欲地对待这些机器人——在这种情况下，“使用成人-儿童性行为的模拟在道德上是可接受的”（Moen and Sterri 2018：377）。Gordon 与 Nyholm 将这一论证表述如下：

只有具备理性和自主性、因而拥有尊严的存在才属于道德共同体。
儿童机器人不具备理性或自主性（因而也没有尊严）。
因此，
儿童机器人不属于道德共同体，因为它们缺乏道德上相关的特征。
因此，
与儿童机器人发生性行为并不构成对直接道德义务的违反。

这一论证可以被改编以消除“机器人芭比困境”。由于没有任何人工道德受体（AMP）是理性且自主的存在，它们也不属于道德共同体的一部分，因此也可以被视为实现目的的手段。在这种情况下，人工谋杀中度相似人工道德受体（Mid-AMPs）将是准许的。我们可以将这一论证表述如下：

机器人芭比不是理性且自主的存在。
人工谋杀任何非理性且自主的存在都是准许的。[因为它们不属于道德共同体。]
因此：
人工谋杀机器人芭比是准许的。

如果命题 3 为真，那么困境中的前提 P3 将为假。

戈登（Gordon）和尼霍姆（Nyholm, 2022）对这类论证提出了一个异议：康德（Kant, 1997）曾主张我们不应将模仿人的事物（如动物）仅仅视为达成目的的手段。因为此类行为可能会使我们变得残忍。（我们将在第 V.1 节详细阐述这一点。）如果这一观点正确，那么尽管机器人芭比不属于道德共同体，也不应被仅仅视为手段。在这种情况下，上述论证的前提 2 是错误的。

IV.1.2 库克的虚构消除论 (Cooke’s fictional dissolution)

库克（Cooke, 2014）会认为，参与虚构错误行为——比如在谋杀之谜晚餐（剧本杀）中扮演“凶手”——只要不代表对该错误行为的
认可
，就是准许的。也就是说，虚构错误行为本身作为一种虚构想象，并不意味着对这些想象内容之准许性的承诺。这是因为，参与虚构想象本质上是通过对虚构作品持有一种“虚构立场”（Lamarque and Olsen 1996），将虚构内容（如虚构谋杀）视为一种“假装游戏”（make-believe）的产物。例如，谋杀之谜晚餐中的“凶手”只是表现得好像实施了谋杀（他们将虚构谋杀视为准许的），同时他们深知自己并没有真正杀人。结果是，他们对现实谋杀的准许性并不负有任何真值上的承诺（alethic commitment）。

这一观点导向了“机器人芭比困境”的第二种非道德主义消除方案。既然人工错误行为是虚构错误行为的一个子集，那么作为虚构想象的人工错误行为也许可以免受道德批评，因为它们并不包含对错误行为的认可。如果是这样，我们可以拒绝前提 P3，因为人工谋杀机器人芭比是一种虚构想象行为，而这种行为本身必然不涉及对想象内容的道德准许性的承诺。因此，在单纯的虚构谋杀案例中，这种消除策略可以表述如下：

如果一个人在虚构地实施某项错误行为时并未认可该行为，那么实施该虚构错误行为是准许的。

一个人通过人工谋杀“机器人芭比”这一行为，并不代表其认可谋杀行为。
因此：
人工谋杀“机器人芭比”是准许的。

换句话说，仅仅实施虚构的错误行为不足以使该行为变得不可准许。如果命题 3 为真，那么困境中的前提 P3 将为假。

巴特尔（Bartel）和克雷马尔迪（Cremaldi, 2018）反对库克的这种非道德主义方法，认为它未能识别出个人对虚构行为做出不同反应时的道德相关性。例如，非道德主义观点无法区分以下两种人：一种是因为参加谋杀之谜派对能满足其真实的谋杀欲望而想扮演“凶手”的人，另一种则是完全没有这种欲望的人。前者的虚构行为与后者不同，因为它具备了“满足个体寻求虚构行为时真实的、不道德的欲望”这一属性。同样地，一个实施人工谋杀的个体可能也是因为这满足了某种不道德的欲望，并可能基于同样的理由在道德上受到谴责。

IV.2 道德主义消除论 (Moralist dissolutions)

道德主义消除论认为，针对中度（Mid-AMPs）和低度相似（Low-AMPs）的人工道德受体实施人工错误行为都是不准许的。这通常是因为所有 AMP 都（至少在某种程度上）模仿了道德受体，而这种相似性本身就足以要求我们对其进行特殊对待。

IV.2.1 麦考密克的美德消除论 (McCormick’s virtue dissolution)

麦考密克（McCormick, 2001）主张，与竞争性的康德主义或功利主义解释相比，亚里士多德式的美德伦理学能更好地解释虚拟暴力的错误性。因为虚拟暴力的总体后果并不明确，功利主义解释难以提供指导；而由于实施虚拟错误行为（相对于真实的错误行为）时违反了哪些道德准则也并不清晰，康德主义的解释也显得乏力。然而，麦考密克认为，美德伦理学“对人的品格有着更广泛的兴趣，而不是关注行为对他人的影响或其对规则的遵循程度” (2001: 285)。

麦考密克论证道，实施虚拟错误行为是错误的，因为这样做会侵蚀我们的美德。

亚里士多德主义者对于“玩暴力电子游戏是否存在错误（如果有的话）”这一问题有一个现成的答案。通过参与对过度、放纵和错误行为的模拟，我们正在培养错误的人格……通过参与此类活动，你对自己造成了伤害，因为你侵蚀了自己的美德，使自己偏离了实现“幸福/繁荣”（eudaimonia）的目标。(2001: 285)

这种对待虚拟错误行为的方式为“机器人芭比困境”提供了一种
道德主义消除
方案，因为人工谋杀芭比娃娃和机器人芭比都涉及对同一种错误行为的模拟。如果实施任何人工错误行为都足以侵蚀一个人的道德品格，那么这两种行为都将是不准许的。我们可以将这一论证表述如下：

实施任何人工错误行为都足以侵蚀一个人的美德。
任何足以侵蚀一个人美德的行为都是不准许的。
人工谋杀芭比娃娃属于一种人工错误行为。
因此：
人工谋杀芭比娃娃是不准许的。

如果命题 4 为真，那么困境中的前提 P1（即人工谋杀芭比是准许的）将为假。

蒂尔森（Tillson）反对这类论证，因为目前尚不清楚为什么实施虚拟错误行为会侵蚀一个人的美德。

一个有美德的人当然不会猥亵儿童；几乎在任何情况下，情境要求都会排除猥亵儿童的行为，有美德的人会对这些要求保持可靠的敏感并依此行动。那么，哪些情境特征禁止了对虚拟儿童的虚拟猥亵呢？除非这个问能得到令人信服的回答，否则人们可以主张：真实的美德与虚拟恋童癖并非互不相容 (2018 : 212)。

蒂尔森向麦考密克发起挑战，要求其解释为什么一个有美德的人会克制这些行为。蒂尔森认为，即便能给出合理的解释，麦考密克的方法也不可能是虚拟错误行为之所以错误的唯一原因。想象有人创造了一个与你极其相似的非玩家角色（NPC），并对其实施虚拟错误行为（如虚拟谋杀、强奸、酷刑等）。他主张，如果以这种方式对待该 NPC 是错误的，那不仅仅是因为行为人伤害了他们自己——他们同时也伤害了
你
。因此，最好采用一种能够同时解释这两类错误的理论。蒂尔森本人就提出了这样一种理论——我们将在接下来详细说明。

IV.2.2 蒂尔森的不尊重消除论 (Tillson’s Disrespectful Dissolution)

蒂尔森论证道：

……模拟对道德显著个体以及对道德显著类别的现实或可能成员的错误行为，是对这些个体以及该类别现实成员的不尊重 (2018 : 216)。

根据蒂尔森的观点，即使实施虚拟错误行为没有产生伤害结果，该行为在初确（pro tanto）意义上仍然是错误的，因为（基于广义的康德主义理由）它是
不尊重
的。重新考虑那个案例：有人创造了一个与你极其相似的 NPC，并着手对其实施虚拟错误行为。在这种情况下，蒂尔森认为你有权感到愤怒，因为此类行为是不尊重的。

蒂尔森（Tillson）论证道，即使你永远不知道有人针对你实施了虚拟错误行为，你仍然受到了冒犯，尽管没有受到实质伤害。此类行为依然是不尊重的。他将这一逻辑从针对个人的虚拟不尊重扩展到对整体人类的不尊重；并得出结论：在缺乏充分理由的情况下，人们应当克制所有的虚拟错误行为。

但在某些情况下，这种抵消因素（即正当理由）可能难以确立。例如，当玩家以轻松的心态对待游戏中的行为（即仅将其视为娱乐），并纯粹为了好玩而实施这些虚拟错误行为时，蒂尔森认为，很难看出某人从中获得的快乐如何能抵消其实施这些行为的错误性：

从此类行为中获得快乐，似乎不仅不能辩解其道德上的问题，反而加剧了问题的严重性。霸凌者及其同伙从戏弄一名智力障碍受害者中获得的快乐（即使受害者不知道自己正在受虐），绝不能为该行为的道德地位辩解，甚至不能减轻其程度，而只会使其恶化。这种初确（pro tanto）的错误如何能被抵消呢？ (2018 : 216)

不过，在某些案例中，此类顾虑可能会被抵消。例如，在虚拟现实环境中训练警察进行道德决策，由于具有教育价值，这可能会证明他们在模拟中实施虚拟错误行为是正当的。

这种关于虚拟错误行为不可准许性的论证，为“机器人芭比困境”提供了另一种
道德主义消除
方案。因为，至少在模拟错误行为的错误性未被抵消的情况下，人工谋杀芭比娃娃和机器人芭比都是不准许的。因为这两者都涉及对同一种错误行为的模拟。我们可以将这一论证表述如下：

实施任何人工错误行为都是对道德受体的不尊重。
不尊重道德受体是不准许的，除非存在充分的抵消因素。
人工谋杀芭比娃娃并不具备充分的抵消因素。
因此：
人工谋杀芭比娃娃是不准许的。

如果命题 4 为真，那么困境中的前提 P1 将为假。

达夫纳尔（Davnall, 2021）基于“电子游戏与书籍或电影之间的区别无法清晰界定”这一主张提出了异议。蒂尔森认为，观看包含错误行为表现的电影（如《辛德勒的名单》）只要观众持有正确的态度，就不一定是不尊重的。然而，达夫纳尔指出，实施虚拟错误行为也应同样适用此逻辑，因为“从逻辑上讲，存在一种可能性，即玩家在没有这种问题态度的情况下玩暴力电子游戏” (2021 : 231)。这种可能性表明，并非所有虚拟错误行为都是不尊重的，这反过来意味着该消除方案中针对“机器人芭比困境”的前提 1 可能并不总是成立。

V. 解决机器人芭比困境

解决“机器人芭比困境”的第二种策略是对其进行
解决
（resolve）。也就是说，论证人工谋杀普通芭比是准许的，但人工谋杀机器人芭比则不然。换言之，我们正在寻找一个反对困境中前提 P2 的论证，即寻找这两项行为之间存在“相关区别”的理由。

V.1 康德的“残暴化”解决论

根据康德（1997 : 240）的观点，我们只对那些能够产生义务的事物负有义务。在这种情况下，我们并不对动物负有任何义务。但这并不意味着我们可以对它们为所欲为。因为我们对人类负有义务，而动物与人类相似。这种相似性非常强，以至于对动物采取残暴行为会使我们更容易对人类采取残暴行为。而我们有义务不让自己变成这样的人。

不难看出，这一论证可以被应用到人工道德受体（AMP）而非动物身上。因为正如动物模仿人类一样，AMP 也是如此。关于对待儿童性爱机器人（Gordon and Nyholm 2022）、人工智能（Turner 2019）、社交机器人（Darling 2016; Friedman 2020）以及非玩家角色（NPC）（Elton 2000; McCormick 2001; Waddington 2007; Ulbricht 2023）的研究中都提出了同样的观点。

这种从动物到 AMP 的逻辑迁移可以被用来解决“机器人芭比困境”。因为如果康德的论证成立，那么似乎有理由认为：一个 AMP 越像道德受体，通过残暴地对待它，你就越有可能变得残暴。正如埃尔顿（Elton）所言：

根据这类推理，踢石头不是问题，但踢那些画了人脸的石头可能就有问题了。此类石头虽然是极其拙劣的人物表现物，但它们终究是表现物，因此据推测，带有让踢石者变得残暴的风险。(2000 : 21)

“变得残暴的可能性”为我们提供了一个潜在的相关区别，可以在人工谋杀普通芭比与机器人芭比之间打下一枚楔子。由于机器人芭比是一个中度相似人工道德受体（Mid-AMP），它模仿道德受体的程度足以让人轻易将其想象为受体。但普通芭比并非如此——作为低相似度人工道德受体（Low-AMP），它与道德受体的相似度要低得多，因此更难将其想象为受体。鉴于此，残暴对待机器人芭比使你变得残暴的可能性，要高于残暴对待普通芭比使你变得残暴的可能性。如果这种可能性上的差异是显著的，那么它就可以充当一个道德上的相关区别。我们可以将这一论证表述如下：

人工谋杀机器人芭比显着更有可能让你变得残暴。
人工谋杀普通芭比并非显着更有可能让你变得残暴。
显着更有可能让你变得残暴的行为是不准许的。[因为在执行这些行为时，你未能履行对自己的道德义务。]
因此：
人工谋杀机器人芭比与人工谋杀普通芭比之间存在相关区别。

如果命题 4 为真，那么困境中的前提 P2 将为假。

正如乌尔布里希特（Ulbricht）所指出的，对这一论证的一个异议是，“关于游戏玩家残暴化的假设，预设了习惯化效应会扩展到游戏情境之外” (2023 : 39)。正如戈登和尼霍姆所观察到的，这种情况是否属实“必须由实证研究来证实” (2022 : 137)。麦考密克还指出，即使残暴对待 AMP 确实增加了我们变得更残暴的可能性，我们仍然不知道在什么节点上“该活动的负面副作用足以证明避开或在道德上谴责该活动是正当的” (2001 : 284)。

您可以进行的下一步：
您是否希望我继续为您翻译
V.2 情感依恋解决论
？该部分将讨论我们与机器人之间建立的心理纽带如何创造出不同的道德边界。

V. 2 科克尔伯格的关系解决论

科克尔伯格（Coeckelbergh 2010）提出，道德考量并非源于事物的内在属性，而是通过社会关系产生的。根据这一观点，一个机器人越是具备例如“交流、玩耍、友好相处以及展示亲社会品质的能力”（Jecker 2024 : 52），我们就越有可能与其建立社会关系。科克尔伯格建议，正因为存在这些关系，此类机器人可能理应获得一定程度的道德考量——尽管它们可能并不具备完全的道德受体资格（2010 : 210）。换句话说，此类机器人可能构成了
准道德受体
（quasi-moral patients）（2010 : 218），拥有某种程度的“间接道德地位”（Coeckelbergh 2021）。

科克尔伯格的关系方法在后续著作中得到了进一步发展。在《培育道德关系》（
Growing Moral Relations
, 2012a）中，他完善了自己的框架，强调道德考量并非固定不变，而是在这些关系实践中动态生成的。同样，科克尔伯格和君克尔（Gunkel 2014）将这种关系方法扩展到了动物身上。该方法也得到了其他学者的补充。例如，杰勒斯（Gellers 2020）强调，在确定机器人的道德（及法律）考量时，需要一种结合了关系因素和内在因素的综合方法。肖勒（Showler）同样主张采用综合方法，他认为如果道德考量仅通过社会关系产生，“那么鲁滨逊·克鲁索（即一个被困荒岛、不与他人发生任何社会关系的人）将失去道德地位”（2024 : 51）。与此类似，杰克尔（Jecker 2024）提出了一种“簇方法”（cluster approach），即包括社会关系存在在内的多个充分条件均能作为道德考量的基础。一些人则采取了更极端的关系方法，普齐奥（Puzio 2024）提出，所有关于道德考量的替代性解释最终都可以归结为社会关系。

关系方法也可以被改编以解决“机器人芭比困境”。如果建立社会关系的可能性随着机器人与人类相似程度的提高而增加，那么我们与中度相似人工道德受体（Mid-AMPs）建立此类关系的可能性要高于低相似度人工道德受体（Low-AMPs）。正如约翰逊和维尔迪基奥（Johnson and Verdicchio）指出的，科克尔伯格的论证“在类人社交机器人的案例中尤其具有说服力，因为这些机器人在外表上比非人类或非社交机器人更接近人类”（2018 : 293）。因为与类人社交机器人的互动更能“共同创造出一种真实关系参与的幻觉”（Coeckelbergh 2021 : 338），使它们成为准道德受体。这种可能性上的差异可以作为一种道德上的相关区别，表述如下：

人工谋杀机器人芭比极有可能涉及对一个准道德受体的虐待。
人工谋杀普通芭比并不太可能涉及对一个准道德受体的虐待。
极有可能涉及虐待准道德受体的行为是不准许的。[因为它们未能对我们与之建立的关系给予应有的道德考量。]
因此：
人工谋杀机器人芭比与人工谋杀普通芭比之间存在相关区别。

如果前提 4 为真，那么机器人芭比困境中的前提 P2 便是错误的。

虽然科克尔伯格的关系方法提供了一个极具吸引力的框架，但它也引发了关于人机交互伦理的更广泛担忧。例如，斯派洛（Sparrow）认为，机器人本质上是无生命的，因此不是爱或悲伤等情感的恰当对象。他声称，建立此类关系是一种在道德上令人不齿的伤感主义（sentimentality）（2002 : 315）。布莱森（Bryson 2010）也表达了类似的担忧，她警告说，选择机器人互动而非人类互动存在道德风险。布莱森认为，机器人互动虽然可预测且风险较低，但远不如人类关系那样丰富和充实，可能导致社会纽带的退化（2010 : 70）。此类批评凸显了过度依赖机器人或对其产生情感依恋的风险，这可能会破坏有意义的人际连接。

V. 3 勒克的“严重性解决论”

为了解决更广泛意义上的“游戏玩家困境”，勒克（Luck 2022）提出，某些虚拟错误行为可能过于
严重
（grave）而不能被轻率对待。“严重性”是指某种类似庄重或肃穆的性质。例如，大屠杀是一个严重的错误行为案例，而将糖果包装纸扔到人行道上则不然。根据勒克的观点，一个虚拟错误行为越写实，它就越严重（2022: 1303）。这似乎与我们对更普遍的错误行为描绘的感受相一致。德·切萨雷（De Cesarei）和科迪斯波蒂（Codispoti）（2008）指出，对袭击图像的描绘越不写实，我们对该图像产生的情绪就越不明显；拉米雷斯（Ramirez）也认为，“模拟错误的非真实实例不太可能被受试者判定为错误”（2020: 151）。

勒克认为，如果一个虚拟错误行为变得过于写实，从而过于严重，那么轻率地对待它可能“是不尊重、无知、迟钝、漫不经心或冒犯性的等”（2022: 1298）。帕特里奇（Partridge）在涉及“表现细节”时也提出了类似的观点，认为虚拟错误行为越详尽，实施它看起来就越不像是一种“‘无害的娱乐’”（2013: 33）。继帕特里奇之后，达纳赫（Danahar）指出，“社会意义越是写实和明确，玩家的反应越是不恰当，其迟钝程度就越高”（2017: 84）。简而言之，虚拟错误行为越写实，实施它的性质就可能越恶劣。

由于虚拟错误行为是人工错误行为的一个子类，这些观点可以被改编以解决“机器人芭比困境”。因为，如果机器人芭比比普通芭比更写实，那么针对机器人芭比实施的错误行为就会更严重。如果这种严重程度上的差异是显著的，那么它就可以作为一种道德上的相关区别。我们可以将这一论证表述如下：

人工谋杀机器人芭比是在过于轻率地对待某事。[因为它与道德受体的高度相似性使得该行为过于严重。]
人工谋杀普通芭比并非在过于轻率地对待某事。
轻率地对待某事（指严重的事物）是不准许的。
因此：
人工谋杀机器人芭比与人工谋杀普通芭比之间存在相关区别。

如果命题 4 为真，那么困境中的前提 P2 便是错误的。

对这一解决论的一个异议是：我们并不总是拥有“虚构错误行为越写实就越严重”的直觉。例如，蒙特菲奥里（Montefiore）和福莫萨（Formosa）提出：

……对一个似乎没人在意的流浪汉遭受羞辱的描绘……在刻画上可能非常精准（即距离过近），但这种……虚构上的临近感似乎并未使其描绘在道德上比其他情况更严重 (2023a : 58)。

这一结果意味着，至少在这些例子中，我们的直觉并不支持现实主义与严重性之间的这种联系。

对该解决论的第二个异议是：这些行为的严重性并没有显著不同，因为它们描绘的是同一种错误行为。例如，蒙特菲奥里和福莫萨提出：

……所涉及的罪行或错误的道德严重性，通常至少与它所应受的惩罚保持一定程度的比例（即便还考虑了威慑或悔改等其他相关因素）。在刑法典中，谋杀通常被认为是非常严重的错误，应受到最严厉的惩罚之一 (2023a : 57)。

如果惩罚与严重性之间的这种联系成立，那么人工谋杀机器人芭比与人工谋杀普通芭比的严重性可能并没有显著差异。这是因为这两种行为涉及同一种错误行为（人工谋杀），如果这发生在现实中，它们将受到同样的惩罚。这一结果似乎与“其中一种行为可能比另一种行为显著更严重”的观点背道而驰。

V. 4 巴特尔的美德解决论

巴特尔（Bartel 2020）追随麦考密克（McCormick 2001）的观点，认为美德伦理学能最好地解释虚拟行为的错误性。巴特尔主张，在特定情况下虚拟行为可能是不道德的，并对其论证做了如下总结：

某些欲望即使未付诸行动，拥有它们也是不道德的。
欲望可以通过现实或幻想来培养。
培养不道德的欲望在道德上是错误的。
电子游戏可以作为个人幻想的道具。
因此，
它们可以被用来培养欲望，无论是道德的还是不道德的。
因此：
当玩电子游戏有助于培养不道德的欲望时，这种行为在道德上是错误的。(2020 : 100)

上述论证的核心在于：
玩家的动机至关重要
。一件道德上准许的虚拟行为与一件不准许的行为之间的区别，归根结底在于该行为是否由不道德的欲望所驱动。如果是，那么执行该行为就会强化与之相关的恶劣行为倾向。在这种情况下，当虚拟谋杀是由无害的欲望（如竞争、探索、享受复杂且高难度的审美体验）驱动时，它是道德准许的；但当它由残忍的欲望（如看到他人受苦的欲望）驱动时，则是道德上不准许的。

为了将此应用于“游戏玩家困境”，巴特尔主张我们必须识别出进一步的区别：玩家参与虚拟谋杀可能是出于广泛的动机，其中一些是“可挽回的”（redeeming），一些则不然；而参与虚拟猥亵儿童的可挽回动机范围则要小得多 (2020 : 138–141)。结果是，在适当的情况下，可能存在无害、可挽回的动机允许玩家参与虚拟谋杀而不受道德谴责，但在类似情况下参与虚拟猥亵儿童则是不可原谅的。

为了解决“机器人芭比困境”，我们可以假设人工谋杀普通芭比与机器人芭比在准许动机的范围上也存在类似的差异。例如，当拔掉机器人芭比的头时，一个人可能是为了享受她痛苦的尖叫声，这将使该行为变得不准许。然而，在拔掉普通芭比的头时，这种动机是无法实现的（因为它不会尖叫）。这种准许动机上的差异可能使我们能够构建如下归纳论证：

人工谋杀普通芭比比人工谋杀机器人芭比拥有更多无害的动机。
只有当人工谋杀 AMP 是出于无害动机时，该行为才是准许的。
因此：
人工谋杀机器人芭比与人工谋杀普通芭比之间存在相关区别。

如果命题 3 为真，那么前提 P2 便是错误的。

对该方法的一个异议是：可挽回动机与不可原谅动机的范围究竟有多大差异，仍有待观察。在巴特尔对电子游戏的分析中，虚拟谋杀比虚拟猥亵儿童拥有更多可挽回动机或许是真的，但在伤害低相似度受体（Low-AMP）与伤害中度相似受体（Mid-AMP）之间的这种差异可能并不那么显著。

VI. 抵制机器人芭比困境

解决“机器人芭比困境”的第三种策略是
抵制
它。这种策略通过论证支撑每个命题的直觉可能缺乏妥当的基础来实现。换句话说，抵制机器人芭比困境旨在削弱赋予该困境中前提 P1 和/或 P3 道德直觉感召力的力量。

虽然即便缺乏直觉支持，P1 和 P3 仍有可能为真 (Luck 2024)，但削弱激发它们的直觉，会使原本急需解决的困境变得不再紧迫。因此，抵制尝试并不寻求“解决”困境，而是主张：如果没有明确合理的道德直觉，我们最初就无法声称存在一个具有哲学意义的道德挑战需要去解决。

VI.1 福莫萨等人的描述性抵制

在“游戏玩家困境”的案例中，福莫萨等人 (Formosa et al. 2023) 在实证支持下提出，与文献中的主张相反，实施虚拟谋杀通常被认为是不道德的行为（尽管比虚拟性侵更容易接受）。因此，这些实证结果可能不利于证明“游戏玩家困境”中 P1 的合理性，因为它缺乏大众直觉的支持。

当应用到“机器人芭比困境”时，人们可能同样会质疑其命题是否具有广泛的大众直觉支持。即，个人是否在直觉上认为人工谋杀普通芭比在准许性上不同于人工谋杀机器人芭比。然而，这一策略看起来并不乐观。

“机器人芭比困境”中的 P1 和 P3 似乎都符合大众直觉。例如，铃木等人 (Suzuki et al. 2015) 的研究表明，当面对疼痛刺激（如切割机器人或人类的手）时，个体对中度相似人工道德受体（Mid-AMPs）的共情程度与对道德受体（人类）的程度相似。因此，相比于普通芭比，个体更有可能在道德上反对人工谋杀机器人芭比，这是合理的。这部分是因为那些外貌更像人类，或表现出更多类人社交行为（如注视或指点）的 AMP，会引发更高程度的
观点采择
（Perspective taking）。观点采择反过来可能会激起人类观察者的共情反应 (Ye et al. 2023)。由此，我们可以合理推断，个体在道德上反对人工谋杀普通芭比的可能性较低。鉴于我们确实在直觉上认为低相似度受体（Low-AMPs）与中度相似受体（Mid-AMPs）在某些方面存在差异，且这些差异很可能激发出具有道德意义的大众直觉，那么关于激发 P1 和 P3 的直觉确实存在的观点表现得理由充分。

VI.2 蒙特菲奥里和福莫萨的规范性抵制

在关于“游戏玩家困境”的文献中，蒙特菲奥里和福莫萨 (2022; 2023a) 抵制该困境的方法是：论证那些根深蒂固的虚拟行为
常规规范
（Conventional norms）可能构成了看似道德直觉的基础。也就是说，虚拟谋杀在常规上的可准许性，以及虚拟性侵在常规上的不可准许性，可能会混淆我们对其
道德准许性
的直觉。这种策略承认了激发 P1 和 P3 的直觉存在，但否认它们在道德层面上具有认识论的正当性。

同样的现象很可能也发生在“机器人芭比困境”中。因为那种“准许人工谋杀普通芭比、但不准许人工谋杀机器人芭比”的直觉，可能并没有妥当的
道德依据
。例如，正如上述研究所表明的，关于人工错误行为准许性的直觉，可能受到某些心理特征的塑造——比如中度相似受体（Mid-AMPs）能以低相似度受体（Low-AMPs）无法做到的方式诱发个体的“观点采择”（perspective taking），使得想象“成为机器人芭比”的感觉要比想象“成为普通芭比”容易得多。然而，单凭“能诱发观点采择”这一点，并不能证明我们基于道德理由将人工谋杀机器人芭比与普通芭比区分开来的直觉是正当的。

换言之，
模仿“道德受体”的特征并不等同于真正具备“道德受体”的地位。

VII. 结语

我们引入了“机器人芭比困境”（Robo-Barbie Dilemma），旨在探讨针对低相似度人工道德受体（Low-AMPs）实施的人工错误行为，是否在道德上与针对中度相似受体（Mid-AMPs）实施的行为有所区别。借鉴关于“游戏玩家困境”的文献，使我们能够探索应对“机器人芭比困境”的新颖方式，并由此提供了一种思考人工错误行为道德性质的方法。

然而，与“游戏玩家困境”相似，目前尚不存在单一且明确的解决方案。考虑到近期机器人工学和人工智能等领域在创建中度相似受体（Mid-AMPs）方面取得的巨大进展，这一点尤为令人担忧。尽管如此，这一新难题为应用伦理学文献提供了一条共同的主线——通过将“人工道德受体”（AMPs）确立为共同的概念分母，将关于电子游戏暴力、性爱机器人、虚构不道德行为以及社交机器人伦理的研究整合在一起。通过这种方式，未来关于 AMP 伦理的研究可以借鉴这些相关文献，为这一紧迫的伦理难题寻求新颖的解决方案。

除了为审视这一难题提供概念资源外，“机器人芭比困境”还在另外两个方面做出了贡献。首先，通过阐明它与“游戏玩家困境”的共同特征，我们将“游戏玩家困境”领域的最新成果引入了 AMP 伦理学。例如，关于“游戏玩家困境”的进一步研究（如勒克的“严重性解决论”）可以被应用于 AMP 伦理学，若非通过这一困境的搭建，这种跨学科应用将难以实现。其次，得益于这一共享框架，关于 AMP 伦理的研究现在也可以同样应用于“游戏玩家困境”。例如，科克尔伯格的关系伦理学可能为“游戏玩家困境”提供潜在方案，因为我们与虚拟儿童建立的社会纽带可能比与虚拟成年人建立的更强。

这种见解的相互交流不仅丰富了两个领域的辩论，也为未来研究如何对待人工道德受体开辟了新途径。因为，正如“游戏玩家困境”对虚拟错误行为的伦理发起了挑战，“机器人芭比困境”同样对 AMP 的伦理发起了挑战。鉴于它们的相似性，我们或许会发现这两个难题共享着相似的解决方案。³

³ 我们对 Alex Fisher, Karim Nader, Immaculate Motsi-Omoijiade, Léa Bourguignon 以及两位匿名评审员为本文发展所做出的宝贵贡献表示感谢。

![图片](articles/images/机器人芭比困境：我们应当如何对待人工道德受体？/3.jpg)

![图片](articles/images/机器人芭比困境：我们应当如何对待人工道德受体？/4.jpg)

![图片](articles/images/机器人芭比困境：我们应当如何对待人工道德受体？/5.jpg)

